{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1kMaIx6HfqEcGbl9bHzfN5domSH3C8UXl",
      "authorship_tag": "ABX9TyMcqzOF5p8Y2my5Sj8XcPw7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HASTAR-CPP/Aionic/blob/main/Aionic-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igXRVDLI1U7h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.signal import butter, filtfilt\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.combine import SMOTETomek\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **Step 1: Load Dataset from Multiple CSV Files**\n",
        "train_path = \"/content/drive/MyDrive/TrainCSV_C23\"\n",
        "test_path = \"/content/drive/MyDrive/TestCSV_C23\"\n",
        "\n",
        "def load_csv_from_folder(folder_path):\n",
        "    all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
        "    df_list = [pd.read_csv(f) for f in all_files]\n",
        "    return pd.concat(df_list, ignore_index=True) if df_list else None"
      ],
      "metadata": {
        "id": "WM_AxYFd2uGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test data\n",
        "train_df = load_csv_from_folder(train_path)\n",
        "test_df = load_csv_from_folder(test_path)\n",
        "\n",
        "print(\"Training dataset shape:\", train_df.shape)\n",
        "print(\"Testing dataset shape:\", test_df.shape)\n"
      ],
      "metadata": {
        "id": "MXo9FG6q9GD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Step 2: Remove Unnecessary Columns**\n",
        "def preprocess_dataframe(df):\n",
        "    df.drop(columns=[\"FCR\", \"ECR\"], errors=\"ignore\", inplace=True)  # Remove wrist sensor data\n",
        "    selected_sensors = [\"FDS\", \"ED\", \"FDI\", \"Movement_Label\"]\n",
        "    df = df[selected_sensors]\n",
        "\n",
        "    finger_movements = [\"Finger Flexion\", \"Finger Extension\", \"Pinch Grip\", \"Open Hand\", \"Close Hand\"]\n",
        "    df = df[df[\"Movement_Label\"].isin(finger_movements)]\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Remove outliers\n",
        "    for sensor in [\"FDS\", \"ED\", \"FDI\"]:\n",
        "        df = df[(df[sensor] > df[sensor].quantile(0.01)) & (df[sensor] < df[sensor].quantile(0.99))]\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "9HhxizYWoghF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = preprocess_dataframe(train_df)\n",
        "test_df = preprocess_dataframe(test_df)\n",
        "\n",
        "print(\"Train dataset after preprocessing:\", train_df.shape)\n",
        "print(\"Test dataset after preprocessing:\", test_df.shape)\n"
      ],
      "metadata": {
        "id": "HSpMuu-WpB73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# **Step 3: Apply Low-Pass Filter**\n",
        "def butter_lowpass_filter(data, cutoff=100, fs=1000, order=4):\n",
        "    nyquist = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyquist\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "for sensor in [\"FDS\", \"ED\", \"FDI\"]:\n",
        "    train_df[sensor] = butter_lowpass_filter(train_df[sensor])\n",
        "    test_df[sensor] = butter_lowpass_filter(test_df[sensor])\n"
      ],
      "metadata": {
        "id": "oZkvoL35pF3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# **Step 4: Feature Extraction (Sliding Window)**\n",
        "window_size = 200\n",
        "stride = 100\n",
        "\n",
        "def extract_features_from_window(window_df):\n",
        "    features = {}\n",
        "    for sensor in [\"FDS\", \"ED\", \"FDI\"]:\n",
        "        data = window_df[sensor]\n",
        "        features[f\"{sensor}_mean\"] = np.mean(data)\n",
        "        features[f\"{sensor}_rms\"] = np.sqrt(np.mean(data**2))\n",
        "        features[f\"{sensor}_var\"] = np.var(data)\n",
        "        features[f\"{sensor}_mav\"] = np.mean(np.abs(data))\n",
        "        features[f\"{sensor}_ssc\"] = np.sum(np.diff(data) ** 2)\n",
        "        features[f\"{sensor}_zc\"] = np.count_nonzero(np.diff(np.sign(data)))\n",
        "        features[f\"{sensor}_wl\"] = np.sum(np.abs(np.diff(data)))\n",
        "\n",
        "    features[\"Movement_Label\"] = window_df[\"Movement_Label\"].iloc[0]\n",
        "    return features\n",
        "\n",
        "def create_feature_dataset(df):\n",
        "    processed_data = []\n",
        "    for i in range(0, len(df) - window_size, stride):\n",
        "        window_df = df.iloc[i:i + window_size]\n",
        "        processed_data.append(extract_features_from_window(window_df))\n",
        "    return pd.DataFrame(processed_data)\n",
        "\n",
        "train_features = create_feature_dataset(train_df)\n",
        "test_features = create_feature_dataset(test_df)\n"
      ],
      "metadata": {
        "id": "OzNwP64cpKcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# **Step 5: Normalize Features**\n",
        "scaler = StandardScaler()\n",
        "feature_columns = train_features.columns[:-1]\n",
        "train_features[feature_columns] = scaler.fit_transform(train_features[feature_columns])\n",
        "test_features[feature_columns] = scaler.transform(test_features[feature_columns])\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n"
      ],
      "metadata": {
        "id": "ggNKzyKBpN4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# **Step 6: Balance the Dataset using SMOTETomek**\n",
        "X_train = train_features[feature_columns].values\n",
        "y_train = train_features[\"Movement_Label\"].values.reshape(-1, 1)\n",
        "X_test = test_features[feature_columns].values\n",
        "y_test = test_features[\"Movement_Label\"].values.reshape(-1, 1)\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.transform(y_test)\n",
        "joblib.dump(encoder, \"encoder.pkl\")\n",
        "\n",
        "smote = SMOTETomek(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "ELZFkVvhpQat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# **Step 7: Save Processed Data**\n",
        "np.savez(\"train_test_data_final.npz\", X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n"
      ],
      "metadata": {
        "id": "4idPjQWZpTBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# **Step 8: Train the ML Model**\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(256, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(64, activation=\"relu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(5, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adamw\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "83JS9q34pU55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# **Step 9: Evaluate Model**\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"✅ Model Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_labels, y_pred))\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "cm = confusion_matrix(y_test_labels, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.categories_[0], yticklabels=encoder.categories_[0])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QKCwIqiupYUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Step 10: Save the Model**\n",
        "model.save(\"trained_emg_model.h5\")\n",
        "print(\"✅ Model saved as 'trained_emg_model.h5'\")\n"
      ],
      "metadata": {
        "id": "GkArXyBypa2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PogVLQQfpcnp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}